\documentclass[11pt, reqno]{amsart}

\edef\restoreparindent{\parindent=\the\parindent\relax}
\usepackage{parskip}
\restoreparindent
\usepackage{bbm}

\usepackage{enumitem}
\usepackage{amsmath, amssymb, mathrsfs}
\usepackage{graphicx}
\usepackage{url}

\usepackage{etoolbox}
\patchcmd{\section}{\scshape}{\Large\scshape}{}{}

\usepackage[width=6.5in,height=9.5in]{geometry}


\usepackage[bookmarks,colorlinks,breaklinks]{hyperref} 
\usepackage{color}
\definecolor{dullmagenta}{rgb}{0.4,0,0.4}   % #660066
\definecolor{darkblue}{rgb}{0,0,0.4}
\hypersetup{linkcolor=darkred,citecolor=blue,filecolor=dullmagenta,urlcolor=darkblue} % coloured links

\setlength{\skip\footins}{2cm}

\begin{document}
	\title{\LARGE CS6390 Project Proposal: REDIRE}
	\author{\it Oliver Richardson \hspace{2em} Maks Cegielski-Johnson }
	\maketitle 
	
	\vspace{-2em}
	
	\section{Description and Approach}
	We are ultimately interested in making a paraphrase generation tool, but we will begin our project by building a paraphrase recognition tool. 
	
	\section{Baseline Approach}
	We will train some simple machine learning classifier with features from PPDB 
	
	\section{Data Sources}
	\subsection{ Penn Paraphrase Database (PPDB)} 
	A large set of parallel English phrases. This dataset appears to be very noisy, but is a large set of paraphrases
	
	Available at \url{http://www.cis.upenn.edu/~ccb/ppdb/} 
		
	\subsection{Microsoft Paraphrasing Corpus (MSRPC) }
	Contains lists of pairs of full sentences, with a label corresponding to whether or not the two are paraphrases.\\
	Available at \url{http://research.microsoft.com/en-us/downloads/607d14d9-20cd-47e3-85bc-a2f65cd28042} 
	
	\subsection{SemEval Data for Paraphrasing (Textual Entailment) }
	
	Available at \url{https://www.cs.york.ac.uk/semeval-2013/task7/index.php%3Fid=data.html}
		
	\section{Evaluation}
	Evaluating the recognition tool should be fairly straightforeward; we have training and test data from MSRPC, with which we can measure accuracies and F-score. As we begin to focus on generation, the process of evaluation will become less straightforeward, but we have several evaluation metrics in mind:
	\begin{enumerate}
		\item Finding a lower bound on accuracy by checking paraphrase datasets to see if we generated (more-or-less) an exact paraphrase. We can score reasonably close paraphrases by using wordnet lookups.
		\item Repurposing our Paraphrase Recognizer for testing -- a tempting first pass, but suffers from two issues: it will probably not be accurate enough to test, and is also not independent. 
		\item \textbf{BLEU} metric for evaluating the quality of machine translations
		\item \textbf{ROUGE} metrics for evaluating the quality of summarizations -- this will require a lot of data, perhaps more than we can obtain reasonably, but 
	\end{enumerate}
	
\end{document}