\documentclass[11pt, reqno]{amsart}
\usepackage[width=6.5in,height=9.5in]{geometry}
%\edef\restoreparindent{\parindent=\the\parindent\relax}
\usepackage{parskip}
%\restoreparindent
%\usepackage{bbm}

%\usepackage{xcolor}
\usepackage{multicol}

\usepackage{enumitem}
\usepackage{amsmath, amssymb, mathrsfs}
\usepackage{graphicx}
\usepackage{url}

\usepackage{float}

\usepackage{etoolbox}
\patchcmd{\section}{\scshape}{\large\scshape}{}{}


\usepackage[bookmarks,colorlinks,breaklinks]{hyperref} 
\usepackage{color}

\newcommand{\mala}{Malakasiotis}

\definecolor{dullmagenta}{rgb}{0.4,0,0.4}   % #660066
\definecolor{darkblue}{rgb}{0,0,0.4}
\definecolor{darkred}{rgb}{0.7,0.2,0.2}
\hypersetup{linkcolor=darkred,citecolor=blue,filecolor=dullmagenta,urlcolor=darkblue} % coloured links

\setlength{\skip\footins}{2cm}

\begin{document}
	\title{REDIRE --- Baseline Report}
	
	\author{\textsc Oliver Richardson \hspace{2em} Maks Cegielski-Johnson}
	
	\maketitle 
	
	\vspace{-2em}
	
	\section{Task}

	Our interest in this problem is two-fold; we are interested in studying both recognition and generation of paraphrases. Hence we will need to discuss our current approaches to both of these problems. 
4	
	The paraphrase recognition task is defined as attempting to determine whether two sentences are paraphrases of each other, returning a \textsc{yes/no} answer.
	
	The paraphrase generation task is defined as being given an input sentence, and trying to generate a paraphrase from that input. We have not implemented this for the baseline yet, but are looking into it.%\textcolor{red}{Deciding correctness is not defined yet.}
	

	
	\section{Architecture}
	Our current implementation (\textsc{sim}) is inspired by the \textsc{init} method in a paper by \mala \cite{malakasiotis2009paraphrase}. For the baseline, we used the Microsoft Research Paraphrase Corpus\cite{msrp} (\textsc{msrp}). We make use of the StanfordNLP framework\cite{manning-EtAl:2014:P14-5} for most of our NLP functionality. The general structure of this implementation relies on transforming a pair of sentences to different linguistic representations and then computing a variety of similarity metrics between each representation pair. This procedure is described below.
	
	\begin{enumerate}
	\item First, read all sentence pairs $(s_1, s_2)$ from a file.
	\item Restructure each pair of sentences $s_1$ and $s_2$ into 10 new string transformations $t_{i,1}$ and $t_{i,2}$, maintaining the original syntactic order of the sentence.
		\begin{multicols}{2}
		\begin{enumerate}
			\item String of the tokens
			\item String of the stems.
			\item String of the POS tags.
			\item String of the soundex codes\cite{soundex}.
			\item String of all the noun tokens
			\item String of all the noun stems
			\item String of all the noun soundex codes.
			\item String of all the verb tokens.
			\item String of all the verb stems.
			\item String of all the verb soundex codes.
		\end{enumerate}
		\end{multicols}
	\item For each string pair $s_1$ and $s_2$, and for each transformation of that string $t_{i,1}$ and $t_{i,2}$, we want to compute the similarity\cite{malakasiotis2007learning} $d_i(t_{i,1},t_{i,2})$, using the metrics 
		\begin{multicols}{2}
		\begin{enumerate}
		\item Levenshtein (word edit)
		\item Jaro-Winkler
		\item Manhattan \& Euclidean
		\item Cosine
		\item $n$-gram ($n$ = 3)
		\item Overlap
		\item Dice coefficient
		\item Jaccard coefficient 
		\end{enumerate}
		\end{multicols}
	\item Computing the similarities between each transformation of the two strings results in 90 features. 
	\item Next, for each pair of sentences $s_1$ and $s_2$, if $s_1$ is longer than $s_2$ we compute the substrings of $s_1$ which have the same length as $s_2$. For each substring $s_1'$ generated, we compute all of the similarities between $s_1'$ and $s_2$. We try to find which substring results in the max average similarity over all the similarity metrics, namely $s_1'^*$. Then for $s_1'^*$ we compute each of the similarities between it and $s_2$, as well as the average of all of them, resulting in 10 features. We do this for the string transformations which result in tokens, stems, POS tags, and soundex codes. This results in an additional 40 features per sentence pair (total of 130 features so far). Note that we can just swap the two strings in the discussion above if $s_2$ is larger than $s_1$. 
	
	\item Finally, we add three additional features to the current 130. We add a boolean feature for whether $s_1$ contains a negation, and a feature for whether $s_2$ contains negation. Then finally, we add a feature for the length ratio, defined as $\frac{\min(|s_1|, |s_2|)}{\max(|s_1|, |s_2|)}$.
	
	\item \mala~ then discusses using a dependency grammar for features (\textsc{dep}), which we found easy to implement as well. We can compute the dependency parses for both sentence $s_1$ and sentence $s_2$. Using the dependency parses, we can calculate the following
	$$R_1 = \frac{|\text{common dependencies}|}{|\text{dependencies in }s_1|} \qquad R_2 = \frac{|\text{common dependencies}|}{|\text{dependencies in }s_2|}$$
	where $R_1$ and $R_2$ can both be used as features. We can then also use the harmonic mean $\frac{2R_1R_2}{R_1 + R_2}$ as the final feature. 
	\end{enumerate}
	
	\section{Experiments}
	
	Having defined how to get our similarity-based features, we can start experimenting with different combinations. First, let us define a baseline experiment, (\textsc{base}), for which we will use the Levenshtein distance and a simple linear classifier. This baseline will allow us to see how a na\"ive approach will work on the our dataset. 
	
	\textsc{sim} is the method discussed above which uses string transformations and string similarity metrics. This experiment will allow us to see how well using only string similarity will work for paraphrasing. \textsc{dep} introduces the dependency grammar for a few features, allowing the use of the structure of the sentence for paraphrase recognition. 
	
	\section{Results}
	
	\begin{table}[H]
	\centering	
	\begin{tabular}{|l||l||l|l|l|}
	\hline
	\textbf{Method} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-Score} \\ \hline
	\textsc{base} & 0.64811 & 0.68442 & 0.87358 & 0.76752 \\ \hline
	\textsc{sim} & 0.66492 & 0.72814 & 0.79163 & 0.75856 \\ \hline
	\textsc{sim} + \textsc{dep} & 0.66492 & 0.73224 & 0.78204 & 0.75632 \\ \hline
	\end{tabular}
	\caption{\textsc{redire} results}\label{tab:result}
	\end{table}
	
	\begin{table}[H]
	\centering
	\begin{tabular}{|l||l||l|l|l|}
	\hline
	\textbf{Method} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-Score} \\ \hline
	\textsc{base} & 0.6904 & 0.7242 & 0.8631 & 0.7876 \\ \hline
	\textsc{init} & 0.7519 & 0.7851 & 0.8631 & 0.8223 \\ \hline
	\textsc{init} + \textsc{wn} & 0.7548 & 0.7891 & 0.8614 & 0.8237 \\ \hline
	\textsc{init} + \textsc{wn} + \textsc{dep} & 0.7935 & 0.7891 & 0.8675 & 0.8288 \\ \hline
	\end{tabular}
	\caption{\mala~results \cite{malakasiotis2009paraphrase}}\label{tab:malak}
	\end{table}
	
	\section{Discussion}
	Table \ref{tab:result} contains all the results from our experiments. Table \ref{tab:malak} contains all the results from the \mala~paper, allowing us to compare how well our implementation did. One notable difference is that our implementation does not use WordNet, while \mala~does. This must be considered when comparing our \textsc{sim + dep} to \mala' \textsc{init + wn + dep}. 
	
	Clearly, following the \mala~ paper has not given us comparable results, netting a 7\% lower F-score using just similarity metrics. The next step would be to determine what is causing the much lower score. However, we are still happy with the progress we currently have. Clearly from Table \ref{tab:malak} we can see that adding WordNet (\textsc{wn}) did not boost performance significantly, so we don't necessarily need to focus our attention on implementing this functionality.
	\section{Conclusions}
		
	Since both \textsc{redire} and \mala~ only use na\"ive word similarity metrics for classification, we are interested in attempting to explore deeper means for paraphrase recognition. Using only similarity means that two sentences which are paraphrases of each other but don't share similar words will not be recognized as paraphrases. We want something that can relate the similarity between two paraphrases other than just the words. 
	
	One way we can attempt to achieve this is through the use of a dependency grammar and WordNet. Seeing as this is what the \mala~paper does as well, we need to consider different ways of representing similarity using both tools. If we can achieve a adequate means of doing so, then not only will this help us boost performance for our paraphrase recognition, but will also allow us to begin to approach the problem of paraphrase generation.
	
	
	\section{Contributions}
	
	\begin{minipage}{.45\textwidth}
	  ~\textbf{Maks Cegielski-Johnson}
	  \begin{itemize}
	  \item Implemented \textsc{sim} feature extraction.
	  \item Implemented \textsc{dep} feature extraction.
	  \end{itemize}
	\end{minipage}% This must go next to `\end{minipage}`
	\begin{minipage}{.55\textwidth}
	  ~\textbf{Oliver Richardson}
	  \begin{itemize}
	  \item Implemented machine learning functionality.
	  \item Improved performance of the architecture.
	  \end{itemize}
	\end{minipage}
	
	\bibliographystyle{plain}
	\bibliography{references}
		
\end{document}